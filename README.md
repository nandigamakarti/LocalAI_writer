# ğŸ§  LocalAI Explainer  
**Explain Any Concept Simply â€” Powered by Your Local LLM**

A clean and privacy-first web app that uses locally running large language models (via Ollama) to break down any concept in simple terms. No cloud APIs. No data leaks. Just pure, offline AI.

---

## ğŸŒŸ App Type

**Concept Explainer** â€“ enter any topic, and get a plain-language explanation using local inference.

---

## âš™ï¸ Features

- ğŸ›¡ï¸ **Fully Local Inference** â€“ no internet, no external APIs; all responses are generated using Ollama on your machine  
- ğŸ§  **Multi-model Support** â€“ switch easily between models like `llama3`, `mistral`, `gemma`, and more  
- ğŸ›ï¸ **Adjustable Temperature** â€“ control creativity with a simple slider  
- ğŸ’¬ **Prompt & Response Interface** â€“ clean, minimal UI with instant feedback  
- ğŸ“ **Local Logging** â€“ every explanation is saved automatically  
- ğŸ“± **Responsive UI** â€“ works smoothly on both desktop and mobile devices  

---

## ğŸš€ Quickstart

### âœ… Prerequisites

- Node.js `v14+`
- npm `v6+`
- Ollama installed and running: [https://ollama.com](https://ollama.com)
- At least one model pulled (e.g., `ollama pull llama3`)

---

## ğŸ› ï¸ Installation & Setup

```bash
# Clone the repo
git clone https://github.com/nandigamakarti/LocalAI_writer.git
cd LocalAI_writer

# Install dependencies
npm install

# Configure environment variables (optional)
cp .env-example .env
```

### ğŸ§  Pull a model with Ollama

```bash
ollama pull llama3  # or mistral, gemma, phi, etc.
```

---

## â–¶ï¸ Run the App

Start the app:

```bash
npm start
```

For development mode (with hot-reloading):

```bash
npm run dev
```

Then open your browser:

```
http://localhost:3000
```

---

## ğŸ§ª Using the App

1. Type any concept into the input box (e.g., â€œQuantum Entanglementâ€)
2. Choose a model (e.g., `llama3`)
3. Adjust the temperature if needed (higher = more creative, lower = more focused)
4. Click â€œExplainâ€
5. The output will be displayed and logged

---

## ğŸ“ Project Structure

```
LocalAI_writer/
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html      # Frontend UI
â”‚   â”œâ”€â”€ script.js       # Client-side JS
â”‚   â””â”€â”€ styles.css      # Basic styling
â”œâ”€â”€ logs/               # Auto-generated explanation logs
â”œâ”€â”€ server.js           # Express API backend
â”œâ”€â”€ .env                # Environment variables
â”œâ”€â”€ .env-example        # Example config
â”œâ”€â”€ package.json        # Dependencies
â””â”€â”€ README.md           # Project documentation
```

---

## ğŸ§© How It Works

1. User inputs a concept and submits the request
2. The backend (`server.js`) sends the prompt to the Ollama local API
3. Ollama runs the selected model and returns a response
4. Response is shown on-screen and saved locally in the `logs/` folder

---

## ğŸ›  Customization

### â• Add More Models

```bash
ollama pull phi
ollama pull codellama
```

Then update the model dropdown in `public/index.html`.

### ğŸ¨ Customize the Prompt Logic

Edit the prompt generation in `server.js` to make responses more formal, casual, or use role-based instructions (e.g., â€œExplain as if to a 12-year-oldâ€).

---

## â— Troubleshooting

- **Ollama not running?** â†’ Start it from terminal: `ollama run llama3`
- **Model missing?** â†’ Run: `ollama pull <model-name>`
- **App not loading?** â†’ Check browser console or terminal logs

---

## ğŸ“„ License

MIT License â€“ use, fork, and modify freely.

---

## ğŸ™Œ Acknowledgements

- [Ollama](https://ollama.com) â€“ for making local LLMs accessible
- [Express](https://expressjs.com) â€“ lightweight backend API
- All open-source contributors and model creators
